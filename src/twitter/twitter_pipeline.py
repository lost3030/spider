#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Twitter å®Œæ•´æµæ°´çº¿ï¼šçˆ¬å– â†’ ä¸Šä¼ OSS â†’ AIåˆ†æ â†’ é£ä¹¦é€šçŸ¥
åˆå¹¶ç‰ˆæœ¬ï¼šå…ˆå®Œæˆçˆ¬å–ï¼Œç„¶åæ‰¹é‡å¤„ç†
"""

from __future__ import annotations

import asyncio
import datetime as dt
import json
import os
import random
import re
import sqlite3
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

import alibabacloud_oss_v2 as oss
import requests
from alibabacloud_oss_v2.models import PutObjectRequest
from openai import OpenAI
from playwright.async_api import async_playwright, Page, Browser

# ==================== é…ç½®åŠ è½½ ====================
def load_secrets():
    """åŠ è½½ secrets.json é…ç½®æ–‡ä»¶"""
    secrets_path = Path("config/secrets.json")
    if secrets_path.exists():
        with open(secrets_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

SECRETS = load_secrets()

# ==================== é…ç½® ====================
# Twitter é…ç½®
TARGET_USER = os.getenv("TWITTER_USER") or SECRETS.get("twitter", {}).get("target_user", "elonmusk")
TARGET_URL = f"https://x.com/{TARGET_USER}"
COOKIE_FILE = Path(os.getenv("TWITTER_COOKIE_FILE", "config/twitter_cookies.json"))
DB_PATH = Path(os.getenv("TWITTER_DB_PATH", "data/twitter.db"))
SCREENSHOT_DIR = Path(os.getenv("TWITTER_SCREENSHOT_DIR", "screenshots"))

# æµè§ˆå™¨é…ç½®
HEADLESS = os.getenv("TWITTER_HEADLESS", "true").lower() == "true"
TIMEOUT = int(os.getenv("TWITTER_TIMEOUT", "60000"))

# æ»šåŠ¨é…ç½®
MAX_SCROLLS = int(os.getenv("TWITTER_MAX_SCROLLS", "5"))
SCROLL_DELAY = int(os.getenv("TWITTER_SCROLL_DELAY", "3000"))
MAX_DETAIL_PAGES = int(os.getenv("TWITTER_MAX_DETAIL_PAGES", "10"))

# OSSé…ç½®ï¼ˆä¼˜å…ˆä»ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä» secrets.jsonï¼‰
OSS_ACCESS_KEY_ID = os.getenv("OSS_ACCESS_KEY_ID") or SECRETS.get("oss", {}).get("access_key_id", "")
OSS_ACCESS_KEY_SECRET = os.getenv("OSS_ACCESS_KEY_SECRET") or SECRETS.get("oss", {}).get("access_key_secret", "")
OSS_BUCKET = os.getenv("OSS_BUCKET") or SECRETS.get("oss", {}).get("bucket", "shenyuan-x")
OSS_REGION = os.getenv("OSS_REGION") or SECRETS.get("oss", {}).get("region", "cn-hangzhou")
OSS_BASE_URL = f"https://{OSS_BUCKET}.oss-{OSS_REGION}.aliyuncs.com/"

# é£ä¹¦é…ç½®ï¼ˆä¼˜å…ˆä»ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä» secrets.jsonï¼‰
FEISHU_WEBHOOK = os.getenv("TWITTER_FEISHU_WEBHOOK") or SECRETS.get("feishu", {}).get("webhook", "")

# AIé…ç½®ï¼ˆä¼˜å…ˆä»ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä» secrets.jsonï¼‰
AI_API_KEY = os.getenv("QIANWEN_API_KEY") or SECRETS.get("qianwen", {}).get("api_key", "")
AI_BASE_URL = os.getenv("QIANWEN_BASE_URL") or SECRETS.get("qianwen", {}).get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1")
AI_MODEL = os.getenv("QIANWEN_MODEL") or SECRETS.get("qianwen", {}).get("model", "qwen-vl-plus")
AI_TIMEOUT = int(os.getenv("QIANWEN_TIMEOUT", "120"))

# AI åˆ†ææ•°æ®åº“
AI_DB_PATH = Path(os.getenv("TWITTER_AI_DB_PATH", "data/twitter_ai.db"))

AI_PROMPT = """
ä½ æ˜¯ä¸€åäº‹ä»¶é©±åŠ¨å‹æŠ•èµ„ä¿¡å·åˆ†æå™¨ã€‚

è¾“å…¥ï¼š
- ä¸€å¼  Elon Musk çš„ X æˆªå›¾ï¼ˆå¯èƒ½åŒ…å«æ–‡å­—ã€å›¾ç‰‡ã€è§†é¢‘æˆ–è½¬å‘ï¼‰

ä»»åŠ¡ï¼š
å°†è¯¥æˆªå›¾å‹ç¼©ä¸ºã€äº¤æ˜“çº§ä¿¡å·ã€‘ï¼Œè€Œä¸æ˜¯å†…å®¹è§£è¯»ã€‚

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š

1. ä¸€å¥è¯æ‘˜è¦summary
- ç”¨ä¸€å¥è¯æ¦‚æ‹¬é©¬æ–¯å…‹æœ¬æ¬¡å‘è¨€çš„æ ¸å¿ƒä¿¡æ¯åŠå…¶æ½œåœ¨å¸‚åœºå«ä¹‰  
- ç¦æ­¢èƒŒæ™¯è§£é‡Šä¸å¤è¿°åŸæ–‡

2. ä¿¡å·ç±»å‹ï¼ˆåªèƒ½é€‰ä¸€ä¸ªï¼‰ signal_type 
A. è¡ŒåŠ¨/å…¬å¸è¡Œä¸ºï¼ˆå›è´­ã€äº§èƒ½ã€è®¢å•ã€å¹¶è´­ç­‰ï¼‰  
B. æ”¿ç­–ç«‹åœºï¼ˆå¯¹å…³ç¨ã€ç›‘ç®¡ã€è´¸æ˜“çš„æ€åº¦ï¼‰  
C. æŠ€æœ¯çªç ´/äº§å“å‘å¸ƒ  
D. æƒ…ç»ª/å£æ°´æˆ˜ï¼ˆä¸ç«äº‰å¯¹æ‰‹/æ”¿åºœçš„å†²çªï¼‰  
E. çº¯ä¸ªäººç”Ÿæ´»/å¨±ä¹ï¼ˆå¯¹å¸‚åœºæ— å½±å“ï¼‰

3. å½±å“æ–¹å‘ direction
- Longï¼ˆåšå¤šï¼‰/ Shortï¼ˆåšç©ºï¼‰/ Neutralï¼ˆä¸­æ€§ï¼‰  
- å¿…é¡»æœ‰æ˜ç¡®æ–¹å‘ï¼Œé™¤éæ˜¯çº¯å¨±ä¹

4. èµ„äº§æ˜ å°„ï¼ˆå¿…å¡«ï¼‰assets
åˆ—å‡ºå—å½±å“çš„å…·ä½“èµ„äº§ï¼ŒæŒ‰å½±å“å¼ºåº¦æ’åºï¼š  
ç¾è‚¡ï¼š
Aè‚¡ï¼š  
- å¦‚æœå½±å“å®½æ³›ï¼ˆå¦‚"ç¾å›½ç§‘æŠ€è‚¡"ï¼‰ï¼Œåªåˆ—æ ¸å¿ƒ3ä¸ª

5. ç½®ä¿¡åº¦ï¼ˆ0-10ï¼‰ confidence
- 0-3ï¼šå™ªéŸ³/ä¸ªäººè§‚ç‚¹ï¼Œä¸å¯æ“ä½œ  
- 4-6ï¼šæœ‰ä»·å€¼ä½†éœ€è§‚å¯Ÿ  
- 7-10ï¼šå¯ç›´æ¥é‡‡å–è¡ŒåŠ¨

6. å¤±æ•ˆæ—¶é—´ï¼ˆå¿…å¡«ï¼‰expiry
- è¯¥ä¿¡å·çš„æ—¶æ•ˆæ€§ï¼ˆå³åˆ»/1å¤©/3å¤©/1å‘¨/1ä¸ªæœˆï¼‰  
- ç¤ºä¾‹ï¼š"2å°æ—¶å†…"ï¼ˆå¦‚ç›˜å‰å‘æ¨å½±å“å¼€ç›˜ï¼‰

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{
  "summary": "",
  "signal_type": "A",
  "direction": "Long",
  "assets": {
    "US": [""],
    "CN": [""]
  },
  "confidence": 7,
  "expiry": "3å¤©"
}

æ³¨æ„ï¼š
- ç¦æ­¢è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œåªè¾“å‡º JSON
- å¦‚æœæˆªå›¾æ˜¯çº¯å¨±ä¹/ç”Ÿæ´»å†…å®¹ï¼Œconfidence è®¾ä¸º 0-2
"""



# ==================== æ•°æ®åº“æ“ä½œï¼ˆçˆ¬è™«éƒ¨åˆ†ï¼‰====================
def ensure_twitter_db() -> sqlite3.Connection:
    """åˆå§‹åŒ–æ¨æ–‡æ•°æ®åº“"""
    DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS tweets (
            id TEXT PRIMARY KEY,
            user_handle TEXT NOT NULL,
            text TEXT NOT NULL,
            is_repost INTEGER DEFAULT 0,
            link TEXT,
            screenshot_path TEXT,
            fetched_at TEXT NOT NULL,
            raw_json TEXT
        );
        """
    )
    # åˆ é™¤æ—§çš„å•åˆ—ç´¢å¼•ï¼Œä½¿ç”¨å¤åˆç´¢å¼•æ›¿ä»£
    conn.execute("DROP INDEX IF EXISTS idx_tweets_user;")
    
    # æ—¶é—´ç´¢å¼•ï¼šç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢
    conn.execute("CREATE INDEX IF NOT EXISTS idx_tweets_fetched ON tweets(fetched_at);")
    
    # å¤åˆç´¢å¼•ï¼šä¼˜åŒ– WHERE user_handle = ? ORDER BY fetched_at æŸ¥è¯¢
    # è¿™æ˜¯ä¸€ä¸ªè¦†ç›–ç´¢å¼•(covering index)ï¼Œé¿å…äº†ä¸´æ—¶æ’åº
    conn.execute("CREATE INDEX IF NOT EXISTS idx_tweets_user_fetched ON tweets(user_handle, fetched_at DESC);")
    
    # æ£€æŸ¥å¹¶æ·»åŠ  screenshot_path åˆ—
    cursor = conn.execute("PRAGMA table_info(tweets);")
    columns = {row[1] for row in cursor.fetchall()}
    if "screenshot_path" not in columns:
        conn.execute("ALTER TABLE tweets ADD COLUMN screenshot_path TEXT;")
        print("[INFO] å·²æ·»åŠ  screenshot_path åˆ—åˆ°æ•°æ®åº“")
    
    conn.commit()
    return conn


def known_tweet_ids(conn: sqlite3.Connection, user_handle: str) -> Set[str]:
    """è·å–å·²å­˜å‚¨çš„æ¨æ–‡ ID"""
    rows = conn.execute(
        "SELECT id FROM tweets WHERE user_handle = ? ORDER BY fetched_at DESC LIMIT 300",
        (user_handle,)
    ).fetchall()
    return {r[0] for r in rows}


def save_tweet(conn: sqlite3.Connection, tweet: Dict[str, Any]) -> bool:
    """ä¿å­˜æ¨æ–‡åˆ°æ•°æ®åº“"""
    fetched_at = dt.datetime.now().isoformat(timespec="seconds")
    try:
        with conn:
            conn.execute(
                """
                INSERT INTO tweets (
                    id, user_handle, text, is_repost, link, screenshot_path, fetched_at, raw_json
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(id) DO UPDATE SET
                    text=excluded.text,
                    is_repost=excluded.is_repost,
                    screenshot_path=excluded.screenshot_path,
                    fetched_at=excluded.fetched_at,
                    raw_json=excluded.raw_json;
                """,
                (
                    tweet["id"],
                    tweet.get("user_handle", ""),
                    tweet.get("text", ""),
                    tweet.get("is_repost", 0),
                    tweet.get("link"),
                    tweet.get("screenshot_path"),
                    fetched_at,
                    json.dumps(tweet, ensure_ascii=False),
                ),
            )
        return True
    except Exception as exc:
        print(f"[WARN] ä¿å­˜æ¨æ–‡å¤±è´¥ {tweet.get('id')}: {exc}")
        return False


# ==================== æ•°æ®åº“æ“ä½œï¼ˆAIå¤„ç†éƒ¨åˆ†ï¼‰====================
def ensure_ai_db() -> sqlite3.Connection:
    """åˆå§‹åŒ–AIåˆ†æç»“æœæ•°æ®åº“"""
    AI_DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(AI_DB_PATH)
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS twitter_ai_results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tweet_id TEXT NOT NULL UNIQUE,
            screenshot_path TEXT,
            oss_url TEXT,
            ai_result TEXT,
            summary TEXT,
            processed_at TEXT NOT NULL
        );
        """
    )
    conn.execute("CREATE INDEX IF NOT EXISTS idx_tweet_id ON twitter_ai_results(tweet_id);")
    # ç´¢å¼•ï¼šä¼˜åŒ–æŒ‰æ—¶é—´æŸ¥è¯¢å·²å¤„ç†çš„æ¨æ–‡
    conn.execute("CREATE INDEX IF NOT EXISTS idx_processed_at ON twitter_ai_results(processed_at);")
    conn.commit()
    return conn


def is_ai_processed(conn: sqlite3.Connection, tweet_id: str) -> bool:
    """æ£€æŸ¥æ¨æ–‡æ˜¯å¦å·²ç»AIåˆ†æè¿‡"""
    # ä½¿ç”¨ UNIQUE ç´¢å¼•ï¼ŒæŸ¥è¯¢é€Ÿåº¦ O(1)
    row = conn.execute(
        "SELECT 1 FROM twitter_ai_results WHERE tweet_id = ? LIMIT 1", (tweet_id,)
    ).fetchone()
    return row is not None


def get_processed_tweet_ids(conn: sqlite3.Connection) -> Set[str]:
    """æ‰¹é‡è·å–å·²å¤„ç†çš„æ¨æ–‡IDï¼ˆä¼˜åŒ–æ€§èƒ½ï¼Œé¿å…å¾ªç¯æŸ¥è¯¢ï¼‰"""
    rows = conn.execute(
        "SELECT tweet_id FROM twitter_ai_results"
    ).fetchall()
    return {r[0] for r in rows}


def save_ai_result(
    conn: sqlite3.Connection,
    tweet_id: str,
    screenshot_path: str,
    oss_url: str,
    ai_result: str,
    summary: str,
    processed_at: str
) -> bool:
    """ä¿å­˜AIåˆ†æç»“æœ"""
    try:
        with conn:
            conn.execute(
                """
                INSERT INTO twitter_ai_results (
                    tweet_id, screenshot_path, oss_url, ai_result, summary, processed_at
                ) VALUES (?, ?, ?, ?, ?, ?)
                ON CONFLICT(tweet_id) DO UPDATE SET
                    oss_url=excluded.oss_url,
                    ai_result=excluded.ai_result,
                    summary=excluded.summary,
                    processed_at=excluded.processed_at;
                """,
                (tweet_id, screenshot_path, oss_url, ai_result, summary, processed_at),
            )
        return True
    except Exception as exc:
        print(f"[WARN] ä¿å­˜AIç»“æœå¤±è´¥ {tweet_id}: {exc}")
        return False


# ==================== Cookie ç®¡ç† ====================
def load_cookies() -> List[Dict[str, Any]]:
    """ä» JSON æ–‡ä»¶åŠ è½½ Cookie"""
    if not COOKIE_FILE.exists():
        raise SystemExit(
            f"âŒ Cookie æ–‡ä»¶ä¸å­˜åœ¨: {COOKIE_FILE}\n"
            f"è¯·å‚è€ƒ twitter_cookies.json.example åˆ›å»ºé…ç½®æ–‡ä»¶"
        )
    
    with open(COOKIE_FILE, "r", encoding="utf-8") as f:
        cookies = json.load(f)
    
    if not cookies:
        raise SystemExit("âŒ Cookie æ–‡ä»¶ä¸ºç©º")
    
    return cookies


async def inject_cookies(page: Page, cookies: List[Dict[str, Any]]) -> None:
    """æ³¨å…¥ Cookie åˆ°æµè§ˆå™¨"""
    await page.context.add_cookies(cookies)
    print(f"[INFO] å·²æ³¨å…¥ {len(cookies)} ä¸ª Cookie")


# ==================== Twitter çˆ¬è™«é€»è¾‘ ====================
async def wait_for_timeline(page: Page, user_handle: str, timeout: int = 30000) -> None:
    """ç­‰å¾…æ—¶é—´çº¿åŠ è½½"""
    try:
        await page.wait_for_selector(
            'article[data-testid="tweet"]',
            timeout=timeout,
            state="visible"
        )
        print(f"[INFO] æ—¶é—´çº¿åŠ è½½æˆåŠŸ")
    except Exception as exc:
        raise SystemExit(f"âŒ æ—¶é—´çº¿åŠ è½½å¤±è´¥ï¼ˆå¯èƒ½éœ€è¦é‡æ–°è·å– Cookieï¼‰: {exc}")


async def collect_tweet_links(page: Page, user_handle: str) -> List[Dict[str, Any]]:
    """æ”¶é›†å½“å‰é¡µé¢çš„æ¨æ–‡é“¾æ¥"""
    articles = await page.query_selector_all('article[data-testid="tweet"]')
    tweets = []
    
    for article in articles:
        try:
            link_elem = await article.query_selector(f'a[href*="/{user_handle}/status/"]')
            if not link_elem:
                continue
            
            href = await link_elem.get_attribute("href")
            if not href:
                continue
            
            full_link = f"https://x.com{href}" if href.startswith("/") else href
            tweet_id = href.split("/status/")[-1].split("?")[0]
            
            tweets.append({
                "id": tweet_id,
                "user_handle": user_handle,
                "link": full_link,
                "is_repost": 0,
            })
        except Exception:
            continue
    
    return tweets


async def smooth_scroll(page: Page) -> None:
    """å¹³æ»‘æ»šåŠ¨"""
    scroll_info = await page.evaluate("""
        () => ({
            currentY: window.scrollY,
            totalHeight: document.body.scrollHeight,
            viewHeight: window.innerHeight
        })
    """)
    
    current_y = scroll_info["currentY"]
    total_height = scroll_info["totalHeight"]
    view_height = scroll_info["viewHeight"]
    
    scroll_distance = view_height * random.uniform(2, 3)
    target_y = min(current_y + scroll_distance, total_height - view_height)
    
    steps = random.randint(8, 12)
    step_distance = (target_y - current_y) / steps
    
    for step in range(steps):
        next_y = current_y + step_distance * (step + 1)
        await page.evaluate(f"window.scrollTo({{ top: {next_y}, behavior: 'smooth' }})")
        await page.wait_for_timeout(random.randint(150, 400))
    
    await page.wait_for_timeout(SCROLL_DELAY)


async def fetch_tweet_detail(page: Page, tweet: Dict[str, Any], screenshot_dir: Path) -> Dict[str, Any]:
    """è¿›å…¥æ¨æ–‡è¯¦æƒ…é¡µï¼Œæå–æ–‡å­—å¹¶æˆªå›¾"""
    tweet_id = tweet["id"]
    link = tweet["link"]
    
    print(f"[INFO] è¿›å…¥è¯¦æƒ…é¡µ: {link}")
    
    try:
        await page.goto(link, timeout=TIMEOUT)
        await page.wait_for_selector('article[data-testid="tweet"]', timeout=15000, state="visible")
        await page.wait_for_timeout(1500)
        
        # å±•å¼€é•¿æ¨æ–‡
        try:
            show_more_btn = page.locator('article[data-testid="tweet"] button').filter(has_text="Show more").first
            if await show_more_btn.is_visible():
                await show_more_btn.click()
                await page.wait_for_timeout(1000)
        except Exception:
            pass
        
        # æå–æ–‡å­—
        try:
            text_locator = page.locator('article[data-testid="tweet"] [data-testid="tweetText"]').first
            text = await text_locator.inner_text() if await text_locator.count() > 0 else ""
            tweet["text"] = text.strip()
        except Exception:
            tweet["text"] = ""
        
        # æˆªå›¾
        try:
            article_locator = page.locator('article[data-testid="tweet"]').first
            screenshot_path = screenshot_dir / f"{tweet_id}.jpg"
            await article_locator.screenshot(path=str(screenshot_path), type="jpeg", quality=90)
            tweet["screenshot_path"] = str(screenshot_path)
            print(f"[INFO] å·²ä¿å­˜æˆªå›¾: {screenshot_path}")
        except Exception as exc:
            print(f"[WARN] æˆªå›¾å¤±è´¥: {exc}")
            tweet["screenshot_path"] = None
        
        await page.wait_for_timeout(random.randint(2000, 4000))
        
    except Exception as exc:
        print(f"[WARN] è·å–æ¨æ–‡è¯¦æƒ…å¤±è´¥ {tweet_id}: {exc}")
        tweet["text"] = ""
        tweet["screenshot_path"] = None
    
    return tweet


async def scrape_new_tweets(user_handle: str, known_ids: Set[str]) -> List[Dict[str, Any]]:
    """çˆ¬å–æ–°æ¨æ–‡ï¼ˆåªå¤„ç†ä¸åœ¨ known_ids ä¸­çš„æ¨æ–‡ï¼‰"""
    cookies = load_cookies()
    all_tweet_links: Dict[str, Dict[str, Any]] = {}
    
    SCREENSHOT_DIR.mkdir(parents=True, exist_ok=True)
    
    async with async_playwright() as p:
        browser: Browser = await p.chromium.launch(headless=HEADLESS)
        context = await browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
        )
        
        page = await context.new_page()
        await inject_cookies(page, cookies)
        
        # é˜¶æ®µ1ï¼šæ”¶é›†æ¨æ–‡é“¾æ¥
        print(f"\n[INFO] ========== é˜¶æ®µ1ï¼šæ”¶é›†æ¨æ–‡é“¾æ¥ ==========")
        await page.goto(TARGET_URL, timeout=TIMEOUT)
        await wait_for_timeline(page, user_handle, timeout=30000)
        
        for scroll_num in range(MAX_SCROLLS):
            print(f"\n[INFO] === ç¬¬ {scroll_num + 1}/{MAX_SCROLLS} æ¬¡æ»šåŠ¨ ===")
            
            current_links = await collect_tweet_links(page, user_handle)
            
            new_count = 0
            for t in current_links:
                if t["id"] not in all_tweet_links:
                    all_tweet_links[t["id"]] = t
                    new_count += 1
            
            print(f"[INFO] æœ¬æ¬¡æ”¶é›† {len(current_links)} æ¡ï¼Œæ–°å¢ {new_count} æ¡ï¼Œæ€»è®¡ {len(all_tweet_links)} æ¡")
            
            if scroll_num < MAX_SCROLLS - 1:
                await smooth_scroll(page)
        
        print(f"\n[INFO] é“¾æ¥æ”¶é›†å®Œæˆï¼Œå…± {len(all_tweet_links)} æ¡æ¨æ–‡")
        
        # è¿‡æ»¤å‡ºæ–°æ¨æ–‡
        new_tweet_links = {tid: t for tid, t in all_tweet_links.items() if tid not in known_ids}
        print(f"[INFO] å…¶ä¸­æ–°æ¨æ–‡ {len(new_tweet_links)} æ¡ï¼ˆå·²æ’é™¤æ•°æ®åº“ä¸­å·²æœ‰çš„ï¼‰")
        
        if not new_tweet_links:
            print(f"[INFO] æ²¡æœ‰æ–°æ¨æ–‡ï¼Œè·³è¿‡è¯¦æƒ…é¡µæŠ“å–")
            await browser.close()
            return []
        
        # é˜¶æ®µ2ï¼šåªå¯¹æ–°æ¨æ–‡è¿›å…¥è¯¦æƒ…é¡µ
        print(f"\n[INFO] ========== é˜¶æ®µ2ï¼šè·å–æ–°æ¨æ–‡è¯¦æƒ…å’Œæˆªå›¾ ==========")
        
        new_tweets = []
        tweet_list = list(new_tweet_links.values())
        
        # é™åˆ¶æ•°é‡
        if len(tweet_list) > MAX_DETAIL_PAGES:
            print(f"[INFO] æ–°æ¨æ–‡ {len(tweet_list)} æ¡ï¼Œé™åˆ¶åªå¤„ç†å‰ {MAX_DETAIL_PAGES} æ¡")
            tweet_list = tweet_list[:MAX_DETAIL_PAGES]
        
        for idx, tweet in enumerate(tweet_list):
            print(f"\n[INFO] === å¤„ç† {idx + 1}/{len(tweet_list)} ===")
            detailed_tweet = await fetch_tweet_detail(page, tweet, SCREENSHOT_DIR)
            new_tweets.append(detailed_tweet)
            
            if (idx + 1) % 5 == 0:
                print(f"[INFO] è¿›åº¦: {idx + 1}/{len(tweet_list)} ({(idx+1)*100//len(tweet_list)}%)")
        
        await browser.close()
    
    return new_tweets


# ==================== OSS ä¸Šä¼  ====================
def upload_to_oss(file_path: str) -> Optional[str]:
    """ä¸Šä¼ æ–‡ä»¶åˆ°OSS"""
    object_name = os.path.basename(file_path)
    oss_url = f"{OSS_BASE_URL}{object_name}"
    
    try:
        credentials_provider = oss.credentials.StaticCredentialsProvider(
            access_key_id=OSS_ACCESS_KEY_ID,
            access_key_secret=OSS_ACCESS_KEY_SECRET
        )
        cfg = oss.config.load_default()
        cfg.credentials_provider = credentials_provider
        cfg.region = OSS_REGION
        
        client = oss.Client(cfg)
        
        with open(file_path, 'rb') as file_obj:
            request = PutObjectRequest(
                bucket=OSS_BUCKET,
                key=object_name,
                body=file_obj
            )
            response = client.put_object(request)
            print(f"[INFO] ä¸Šä¼ æˆåŠŸ: {object_name}, ETag: {response.etag}")
        
        return oss_url
    
    except Exception as exc:
        error_msg = str(exc)
        if "FileImmutable" in error_msg or "ObjectAlreadyExists" in error_msg:
            print(f"[WARN] æ–‡ä»¶å·²å­˜åœ¨äºOSS: {object_name}ï¼Œä½¿ç”¨ç°æœ‰URL")
            return oss_url
        else:
            print(f"[ERROR] OSSä¸Šä¼ å¤±è´¥ {file_path}: {exc}")
            return None


# ==================== AI åˆ†æ ====================
def analyze_screenshot(image_url: str) -> Dict[str, Any]:
    """è°ƒç”¨AIåˆ†ææˆªå›¾"""
    client = OpenAI(api_key=AI_API_KEY, base_url=AI_BASE_URL, timeout=AI_TIMEOUT)
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=AI_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": AI_PROMPT},
                            {"type": "image_url", "image_url": {"url": image_url}},
                        ],
                    }
                ],
            )
            
            ai_text = response.choices[0].message.content
            full_response = response.model_dump_json(indent=2)
            
            return {
                "success": True,
                "ai_text": ai_text,
                "full_response": full_response
            }
        
        except Exception as exc:
            print(f"[WARN] AIåˆ†æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {exc}")
            if attempt < max_retries - 1:
                time.sleep(2)
            else:
                return {"success": False, "error": str(exc)}


def extract_summary(ai_text: str) -> str:
    """ä»AIå“åº”ä¸­æå–æ‘˜è¦"""
    try:
        json_match = re.search(r'\{.*\}', ai_text, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            return data.get("summary", "")[:100]
    except Exception:
        pass
    
    if "summary" in ai_text.lower():
        lines = ai_text.split("\n")
        for line in lines:
            if "summary" in line.lower() and ":" in line:
                summary = line.split(":", 1)[1].strip().strip('"')
                summary = re.sub(r'\s+', ' ', summary)
                return summary
    
    return ai_text[:100] if ai_text else "æ— æ‘˜è¦"


# ==================== é£ä¹¦é€šçŸ¥ ====================
def format_ai_result(ai_text: str, image_url: str) -> str:
    """å°†AIåˆ†æç»“æœæ ¼å¼åŒ–ä¸ºå‹å¥½çš„æ–‡æœ¬"""
    try:
        # å°è¯•è§£æ JSON
        json_match = re.search(r'\{.*\}', ai_text, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            
            # ä¿¡å·ç±»å‹æ˜ å°„
            signal_types = {
                "A": "ğŸ“Š è¡ŒåŠ¨/å…¬å¸è¡Œä¸º",
                "B": "ğŸ›ï¸ æ”¿ç­–ç«‹åœº",
                "C": "ğŸš€ æŠ€æœ¯çªç ´/äº§å“å‘å¸ƒ",
                "D": "ğŸ’¬ æƒ…ç»ª/å£æ°´æˆ˜",
                "E": "ğŸ® çº¯å¨±ä¹"
            }
            
            # æ–¹å‘æ˜ å°„
            direction_icons = {
                "Long": "ğŸ“ˆ åšå¤š",
                "Short": "ğŸ“‰ åšç©º",
                "Neutral": "â– ä¸­æ€§"
            }
            
            # ç½®ä¿¡åº¦è¯„çº§
            confidence = data.get("confidence", 0)
            if confidence >= 7:
                confidence_text = f"â­â­â­ é«˜ç½®ä¿¡åº¦ ({confidence}/10)"
            elif confidence >= 4:
                confidence_text = f"â­â­ ä¸­ç­‰ç½®ä¿¡åº¦ ({confidence}/10)"
            else:
                confidence_text = f"â­ ä½ç½®ä¿¡åº¦ ({confidence}/10) - å™ªéŸ³"
            
            # æ„å»ºå‹å¥½çš„æ–‡æœ¬
            formatted = f"""ğŸ“ æ‘˜è¦
{data.get('summary', 'æ— ')}

ğŸ·ï¸ ä¿¡å·ç±»å‹
{signal_types.get(data.get('signal_type', 'E'), 'æœªçŸ¥')}

ğŸ“Š å½±å“æ–¹å‘
{direction_icons.get(data.get('direction', 'Neutral'), 'ä¸­æ€§')}

ğŸ’¼ å—å½±å“èµ„äº§
"""
            
            # æ·»åŠ ç¾è‚¡èµ„äº§
            assets = data.get('assets', {})
            us_assets = assets.get('US', [])
            if us_assets:
                formatted += f"ğŸ‡ºğŸ‡¸ ç¾è‚¡ï¼š{', '.join(us_assets)}\n"
            else:
                formatted += "ğŸ‡ºğŸ‡¸ ç¾è‚¡ï¼šæ— ç›´æ¥å½±å“\n"
            
            # æ·»åŠ Aè‚¡èµ„äº§
            cn_assets = assets.get('CN', [])
            if cn_assets:
                formatted += f"ğŸ‡¨ğŸ‡³ Aè‚¡ï¼š{', '.join(cn_assets)}\n"
            else:
                formatted += "ğŸ‡¨ğŸ‡³ Aè‚¡ï¼šæ— ç›´æ¥å½±å“\n"
            
            # æ·»åŠ ç½®ä¿¡åº¦
            formatted += f"\n{confidence_text}\n"
            
            # æ·»åŠ å¤±æ•ˆæ—¶é—´
            expiry = data.get('expiry', 'æœªçŸ¥')
            formatted += f"\nâ° ä¿¡å·æ—¶æ•ˆï¼š{expiry}\n"
            
            # æ·»åŠ é£é™©æç¤º
            risk = data.get('risk', 'æ— ')
            if risk and risk != 'æ— ':
                formatted += f"\nâš ï¸ å…³é”®é£é™©\n{risk}\n"
            
            # æ·»åŠ æˆªå›¾é“¾æ¥
            formatted += f"\nğŸ–¼ï¸ æˆªå›¾ï¼š{image_url}"
            
            return formatted
    
    except Exception as e:
        print(f"[WARN] æ ¼å¼åŒ–AIç»“æœå¤±è´¥: {e}")
    
    # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
    return f"ğŸ”” é©¬æ–¯å…‹æ¨æ–‡åˆ†æ\n\n{ai_text}\n\nğŸ–¼ï¸ æˆªå›¾ï¼š{image_url}"


def send_to_feishu(title: str, image_url: str, text: str) -> bool:
    """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦ï¼ˆæ ¼å¼åŒ–åçš„å¯Œæ–‡æœ¬ï¼‰"""
    if not FEISHU_WEBHOOK:
        print("[WARN] FEISHU_WEBHOOK æœªé…ç½®ï¼Œè·³è¿‡é£ä¹¦é€šçŸ¥")
        return False
    
    # æ ¼å¼åŒ– AI ç»“æœ
    formatted_text = format_ai_result(text, image_url)
    
    payload = {
        "msg_type": "text",
        "content": {
            "text": formatted_text
        }
    }
    
    try:
        response = requests.post(
            FEISHU_WEBHOOK,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=10
        )
        response.raise_for_status()
        print(f"[INFO] é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ")
        return True
    except Exception as exc:
        print(f"[ERROR] é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥: {exc}")
        return False


# ==================== å¤„ç†æµç¨‹ ====================
def process_new_tweets(new_tweets: List[Dict[str, Any]], ai_conn: sqlite3.Connection) -> int:
    """å¤„ç†æ–°æ¨æ–‡ï¼šä¸Šä¼ OSSã€AIåˆ†æã€å‘é€é£ä¹¦"""
    if not new_tweets:
        print(f"[INFO] æ²¡æœ‰æ–°æ¨æ–‡éœ€è¦å¤„ç†")
        return 0
    
    # è¿‡æ»¤å‡ºæœ‰æˆªå›¾çš„æ¨æ–‡
    tweets_with_screenshots = [t for t in new_tweets if t.get("screenshot_path")]
    
    if not tweets_with_screenshots:
        print(f"[INFO] æ²¡æœ‰æˆªå›¾éœ€è¦å¤„ç†")
        return 0
    
    print(f"\n[INFO] ========== å¼€å§‹å¤„ç† {len(tweets_with_screenshots)} ä¸ªæ–°æˆªå›¾ ==========")
    
    # ã€æ€§èƒ½ä¼˜åŒ–ã€‘æ‰¹é‡æŸ¥è¯¢å·²å¤„ç†çš„æ¨æ–‡IDï¼Œé¿å…å¾ªç¯ä¸­é¢‘ç¹æŸ¥è¯¢æ•°æ®åº“
    processed_ids = get_processed_tweet_ids(ai_conn)
    print(f"[INFO] æ•°æ®åº“ä¸­å·²æœ‰ {len(processed_ids)} æ¡AIåˆ†æè®°å½•")
    
    processed_count = 0
    
    for idx, tweet in enumerate(tweets_with_screenshots):
        tweet_id = tweet["id"]
        screenshot_path = tweet["screenshot_path"]
        
        print(f"\n[INFO] === å¤„ç† {idx + 1}/{len(tweets_with_screenshots)}: {tweet_id} ===")
        
        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘ä½¿ç”¨å†…å­˜ä¸­çš„ Set æ£€æŸ¥ï¼ŒO(1) å¤æ‚åº¦
        if tweet_id in processed_ids:
            print(f"[INFO] æ¨æ–‡ {tweet_id} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
            continue
        
        # 1. ä¸Šä¼ OSS
        print(f"[INFO] ä¸Šä¼ åˆ°OSS...")
        oss_url = upload_to_oss(screenshot_path)
        if not oss_url:
            print(f"[ERROR] OSSä¸Šä¼ å¤±è´¥ï¼Œè·³è¿‡")
            continue
        
        print(f"[INFO] OSS URL: {oss_url}")
        
        # 2. AIåˆ†æ
        print(f"[INFO] AIåˆ†æä¸­...")
        ai_result = analyze_screenshot(oss_url)
        
        if not ai_result["success"]:
            print(f"[ERROR] AIåˆ†æå¤±è´¥ï¼Œè·³è¿‡")
            continue
        
        ai_text = ai_result["ai_text"]
        full_response = ai_result["full_response"]
        
        print(f"[INFO] AIåˆ†æå®Œæˆ")
        print(f"[INFO] AIè¿”å›: {ai_text[:150]}...")
        
        # 3. æå–æ‘˜è¦
        summary = extract_summary(ai_text)
        print(f"[INFO] æ‘˜è¦: {summary}")
        
        # 4. ä¿å­˜ç»“æœ
        processed_at = dt.datetime.now().isoformat(timespec="seconds")
        if save_ai_result(ai_conn, tweet_id, screenshot_path, oss_url, full_response, summary, processed_at):
            print(f"[INFO] å·²ä¿å­˜åˆ°AIæ•°æ®åº“")
        
        # 5. å‘é€é£ä¹¦
        print(f"[INFO] å‘é€é£ä¹¦é€šçŸ¥...")
        send_to_feishu(title=summary, image_url=oss_url, text=ai_text)
        
        processed_count += 1
    
    return processed_count


# ==================== ä¸»æµç¨‹ ====================
async def main():
    """ä¸»æµç¨‹ï¼šçˆ¬å– â†’ å¤„ç† â†’ é€šçŸ¥"""
    print(f"=" * 60)
    print(f"Twitter å®Œæ•´æµæ°´çº¿å¯åŠ¨")
    print(f"=" * 60)
    print(f"[INFO] ç›®æ ‡ç”¨æˆ·: @{TARGET_USER}")
    print(f"[INFO] æ¨æ–‡æ•°æ®åº“: {DB_PATH}")
    print(f"[INFO] AIæ•°æ®åº“: {AI_DB_PATH}")
    print(f"[INFO] æˆªå›¾ç›®å½•: {SCREENSHOT_DIR}")
    print(f"[INFO] OSS Bucket: {OSS_BUCKET}")
    print(f"[INFO] é£ä¹¦ Webhook: {FEISHU_WEBHOOK}")
    
    # éªŒè¯é…ç½®
    if not OSS_ACCESS_KEY_ID or not OSS_ACCESS_KEY_SECRET:
        print("[ERROR] OSSé…ç½®ç¼ºå¤±")
        return
    
    if not AI_API_KEY:
        print("[ERROR] AI API KEY æœªé…ç½®")
        return
    
    try:
        # ========== æ­¥éª¤1ï¼šçˆ¬å–æ–°æ¨æ–‡ ==========
        print(f"\n{'='*60}")
        print(f"æ­¥éª¤1ï¼šçˆ¬å–æ–°æ¨æ–‡")
        print(f"{'='*60}")
        
        twitter_conn = ensure_twitter_db()
        known_ids = known_tweet_ids(twitter_conn, TARGET_USER)
        print(f"[INFO] æ•°æ®åº“ä¸­å·²æœ‰ {len(known_ids)} æ¡æ¨æ–‡")
        
        new_tweets = await scrape_new_tweets(TARGET_USER, known_ids)
        print(f"[INFO] æœ¬æ¬¡çˆ¬å–åˆ° {len(new_tweets)} æ¡æ–°æ¨æ–‡")
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        saved_count = 0
        for tweet in new_tweets:
            if save_tweet(twitter_conn, tweet):
                saved_count += 1
        
        print(f"[INFO] å·²ä¿å­˜ {saved_count} æ¡æ¨æ–‡åˆ°æ•°æ®åº“")
        twitter_conn.close()
        
        if not new_tweets:
            print(f"\n[INFO] æ²¡æœ‰æ–°æ¨æ–‡ï¼Œæµç¨‹ç»“æŸ")
            return
        
        # ========== æ­¥éª¤2ï¼šAIå¤„ç†æ–°æ¨æ–‡ ==========
        print(f"\n{'='*60}")
        print(f"æ­¥éª¤2ï¼šAIå¤„ç†æ–°æ¨æ–‡")
        print(f"{'='*60}")
        
        ai_conn = ensure_ai_db()
        processed_count = process_new_tweets(new_tweets, ai_conn)
        ai_conn.close()
        
        # ========== å®Œæˆ ==========
        print(f"\n{'='*60}")
        print(f"æµç¨‹å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"[INFO] æ–°æ¨æ–‡: {len(new_tweets)} æ¡")
        print(f"[INFO] å·²å¤„ç†: {processed_count} æ¡")
        print(f"[INFO] æ¨æ–‡æ•°æ®åº“: {DB_PATH}")
        print(f"[INFO] AIæ•°æ®åº“: {AI_DB_PATH}")
    
    except KeyboardInterrupt:
        print(f"\n[INFO] ç”¨æˆ·ä¸­æ–­")
    except Exception as exc:
        print(f"\n[ERROR] ç¨‹åºå¼‚å¸¸: {exc}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
